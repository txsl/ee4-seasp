\documentclass[./main.tex]{subfiles} 
\begin{document}

\subsection{Adaptive Step Sizes}

\subsubsection{Implemented GASS Algorithms}
Gradient Adaptive Step-Size Algorithms are designed to change the step size as the algorithm progresses. They all rely on optimising gradient descent so as to iterate quickly down a steep gradient, and then remain very stable when in steady state At the start, a large step size is needed in order for the algorithm to converge quickly, but during steady state a small step size helps to reduce the steady state error (as we have observed in previous parts of this coursework).

The LMS algorithm remains the same in structure, but $\mu$ now becomes $\mu(n)$, thus becoming $  \mathbf{w}(n+1) = \mathbf{w}(n) + \mu(n) e(n) \mathbf{x}(n) $. The algorithm to update the weight is defined as $ \mu(n+1) = \mu(n) + \rho e(n) \mathbf{x}^T(n) \boldsymbol{\psi}(n)$. $ \boldsymbol{\psi}(n) $ is defined by three different GASS algorithms, listed below:

\begin{description}
	\item[Matthews \& Xie] $  \boldsymbol{\psi}(n) = e(n-1) \mathbf{x}(n-1) $
	\item[Ang \& Farhang] $  \boldsymbol{\psi}(n) = \alpha  \boldsymbol{\psi}(n - 1) +  e(n-1) \mathbf{x}(n-1) $
	\item[Benveniste] $ \boldsymbol{\psi} = [ \mathbf{I} - \mu(n-1) \mathbf{x}(n-1) \mathbf{x}^T(n-1)  ] \boldsymbol{\psi}(n - 1) + e(n-1) \mathbf{x}(n-1) $
\end{description}

Figure \ref{fig:3_2_a_alpha} shows the Ang \& Farhang method with different values of $\alpha$. If $ \alpha $ is set too high, there runs a risk of the system becoming unstale. We can see this when $\alpha = 0.9$.

Figure \ref{fig:3_2_a} shows the three different variants. The winner appears to be the Benveniste algorithm, which overtakes everyt other algorithm to come in to steady state first. This is followed closely by the Ang \& Farhang filter, and lastly Matthews \& Xie. Overall, they clearly able to converge faster than the standard LMS algorithm, whilst at the same time remaining with a low steady state error.

\begin{figure}[h]
	\centering 
	\resizebox{\textwidth}{!}{\input{fig/3/3_2_a_alphas.tikz}}
	\caption{\textit{LMS filter error (dB) for the given AR process, averaged over 100 independent trials}}
	\label{fig:3_2_a_alpha}
\end{figure}

\begin{figure}[h]
	\centering 
	\resizebox{\textwidth}{!}{\input{fig/3/3_2_a.tikz}}
	\caption{\textit{LMS filter error (dB) for the given AR process, averaged over 100 independent trials}}
	\label{fig:3_2_a}
\end{figure}



\subsubsection{NLMS Algorithm}

The NLMS algorithm is a modification to the LMS algorithm which normalises the power of the input, which for a particularly noisy (in the power spectrum) input signal means the filter can still get to a stable output, which may not happen with the standard LMS filter, or at least not without manually adjusting the step size.

Given the update equation $ \mathbf{w}(n+1) = \mathbf{w}(n) + \mu e_p(n) \mathbf{x}(n) $ and the \textit{a posteriori} relationship given: $  e_p(n) = d(n) - \mathbf{x}^T(n) \mathbf{w}(n+1) $, we can express $ \bigtriangleup \mathbf{w}(n) = \mathbf{w}(n+1) - \mathbf{w}(n) $ to allow us to equate the update equation to the standard NLMS form. 

First, we expand the given relationship of $  e_p(n)$:
\begin{subequations}
	\begin{align}
		e_p(n) &= d(n) - \mathbf{x}^T(n) \mathbf{w}(n+1) \\
		e_p(n) &= d(n) - \mathbf{x}^T(n)  \mathbf{w}(n) - \mu \mathbf{x}^T(n) e_p(n) \mathbf{x}(n) && \text{Expanding the $\mathbf{w}(n+1)$ term} \\
		e_p(n) (1 + \mu \lVert \mathbf{x}(n) \rVert^2 ) &= d(n) - \mathbf{x}^T(n)\mathbf{w}(n) \\
		e_p(n) & = \frac{e(n)}{1 + \mu \lVert \mathbf{x}(n) \rVert^2 }
	\end{align}
\end{subequations}

We now substitute this in to the update equation:
\begin{subequations}
	\begin{align}
	\mathbf{w}(n+1) &= \mathbf{w}(n) + \mu e_p(n) \mathbf{x}(n) \\
	\mathbf{w}(n+1) &= \mathbf{w}(n) + \frac{\mu e(n) \mathbf{x}(n)}{1 + \mu \lVert \mathbf{x}(n) \rVert^2 } \\
	\mathbf{w}(n+1) &= \mathbf{w}(n) + \mu e(n) \left[ \frac{ 1 }{1 + \mu \lVert \mathbf{x}(n) \rVert^2} \right] \mathbf{x}(n) \\
	%
	%\bigtriangleup \mathbf{w}(n) &= \mu e(n) \left[ \frac{ 1 + \mu \lVert \mathbf{x}(n) \rVert^2 - \mu \lVert \mathbf{x}(n) \rVert^2 }{1 + \mu \lVert \mathbf{x}(n) \rVert^2} \right] \mathbf{x}(n) \\
	%
	%\bigtriangleup \mathbf{w}(n) &= \mu e(n) \left[ 1 - \mu \frac{ \lVert \mathbf{x}(n) \rVert^2 }{1 + \mu \lVert \mathbf{x}(n) \rVert^2} \right] \mathbf{x}(n) \\
	%
	%\mathbf{w}(n+1) &= \mathbf{w}(n) + e(n) \left[ \frac{\mu ( 1 + \mu  \lVert \mathbf{x}(n)\rVert^2 - \mu \lVert \mathbf{x}(n)\rVert^2 )}{1 + \mu \lVert \mathbf{x}(n)\rVert^2} \right] \mathbf{x}(n) \\
	%
	%\mathbf{w}(n+1) &= \mathbf{w}(n) + e(n) \left[ \frac{\mu + \mu^2  \lVert \mathbf{x}(n)\rVert^2 - \mu^2 \lVert \mathbf{x}(n)\rVert^2}{1 + \mu \lVert \mathbf{x}(n)\rVert^2} \right] \mathbf{x}(n) \\
	%
	\mathbf{w}(n+1) &= \mathbf{w}(n) + \left[ \frac{1}{\frac{1}{\mu} + \mathbf{x}^T(n)\mathbf{x}(n) } \right] e(n) \mathbf{x}(n)  \label{subeq:3_2_b}	\end{align}
\end{subequations}

We can see that the last line, equation \ref{subeq:3_2_b} now has the same form (and thus is equivalent to, other than the factors which are discussed below) as the NLMS update equation:
$$ \mathbf{w}(n+1) = \mathbf{w}(n) + \frac{\beta}{\epsilon + \mathbf{x}^T(n)\mathbf{x}(n)} e(n) \mathbf{x}(n) $$

Thus from the comparison of forms, we can equate $ \beta = 1 $ and $ \epsilon = \frac{1}{\mu} $.

\subsubsection{GNGD Algorithm}

The GNGD Algorithm is a specific variety of NLMS filter. Figure \ref{fig:3_2_c} compares its performance to that of the best performing GASS algorithm we saw, the Benveniste algorithm. The GNGD significantly outperforms the Benveniste algorithm at reaching steady state.

Comparing the complexity of each algorithm, they both have vector/matrix multiplications in them. The Benvenieste has marginally more complex mutltiplications, but fundamentally, both algorithms are of order of $N^2$ complexity. Considering their similar computational overhead, the GNGD appears the better algorithm since it is able to reach steady state significantly faster than the Benveniste algorithm.

\begin{figure}[h]
	\centering 
	\resizebox{\textwidth}{!}{\input{fig/3/3_2_c.tikz}}
	\caption{\textit{LMS filter error (dB) for the given AR process, averaged over 100 independent trials}}
	\label{fig:3_2_c}
\end{figure}

\end{document}


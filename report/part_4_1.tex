\documentclass[./main.tex]{subfiles} 
\begin{document}

\section{Widely Linear Filtering and Adaptive Spectral Estimation}

\subsection{Complex LMS and Widely Linear Modelling}

\subsubsection{The CLMS and ACLMS}
We are introduced to the Complex LMS, which is a modification of the LMS filter, but designed to operate with complex signals. The $ \mathbf{w}$ weight term is replaced with $ \mathbf{h}$, and the estimation is now $ \hat{y}(n) = \mathbf{h}^H(n) \mathbf{x}(n) $, with the weight update being performed as $ \mathbf{h}(n+1) = \mathbf{h}(n) + \mu e^{\ast}(n)\mathbf{x}(n) $.

We are also introduced to the Augmented CLMS, which is designed to identify the second order (if there is any) statistical relationship of the input and output. It is effectively an extension of the CLMS, and adds the weights $ \mathbf{g}$, in addition to $ \mathbf{h}$ as defined for the CLMS. The esimtation equation becomes  $ \hat{y}(n) = \mathbf{h}^H(n) \mathbf{x}(n) + \mathbf{g}^H(n) \mathbf{x}^\ast(n) $. The next estimated of $ \mathbf{g}$ is defined as $ \mathbf{g}(n+1) = \mathbf{g}(n) + \mu e^{\ast}(n)\mathbf{x}\ast(n) $.

We generated a WLMA(1) (Widely Linear Moving Average) process which is defined as $ y(n) = x(n) + b_1 x(n-1) + b_2 x^{\ast}(n-2) $, where $ x \sim \mathcal{N} (0,1) $. The coefficients are defined as $ b_1 = 1.5 + 1j $ and $ b_2 = 2.5 - 0.5j$. We run this process through both the CLMS and ACLMS filters. 100 indepdent sets of noise were generated and run through both filters (identical signals, to ensure a fair comparison), with the mean of at each iteration taken. Figure \ref{fig:4_1_a_clms_err} shows the learning error. We can see that even in steady state, there appears to be significant modelling error, sitting at 8dB. This is also apparent in figure \ref{fig:4_1_a_clms_weights} where we can see the estimated weights of $\mathbf{h}$. We are unable to see the values of coefficients related to $ b_2 $.

\begin{figure}[h]
	\centering
	\begin{subfigure}[b]{0.49\textwidth}
		\resizebox{\textwidth}{!}{\input{fig/4/4_1_a_clms.tikz}}
		\caption{\textit{Learning Curve of the filter, defined as $ 10 \log|e(n)|^2 $}}
		\label{fig:4_1_a_clms_err}
	\end{subfigure}
	~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
	%(or a blank line to force the subfigure onto a new line)
	\begin{subfigure}[b]{0.49\textwidth}
	 \resizebox{\textwidth}{!}{\input{fig/4/4_1_a_clms_weights.tikz}}
		\caption{\textit{Esimated Weights (both real and imaginary components)}}
		\label{fig:4_1_a_clms_weights}
	\end{subfigure}
	\caption{\textit{Learning Curve and Estimated Weights from the CLMS fitler}}
\end{figure}

Figure \ref{fig:4_1_a_aclms} shows the same plots but this time from the ACLMS filter. We can immediately see that the learning curve in figure \ref{fig:4_1_a_aclms_err} is significantly better than those seen in figure \ref{fig:4_1_a_clms_err} with the CLMS filter. Since we are modelling an WLMA process, the output can be determined directly from the input (unlike with a `next step' estimator), so we expect the error to be very small. This is also evident in the weights in figure \ref{fig:4_1_a_aclms_weights}. We also observe that we can see 4 weights (the real and imaginary components of both $ \mathbf{g}$ and $ \mathbf{h}$), as opposed to the two seen in the CLMS. We can observe that the values associated with $ b_2 $ are represented by $ \mathbf{g} $.

\begin{figure}[h]
	\centering
	\begin{subfigure}[b]{0.49\textwidth}
		\resizebox{\textwidth}{!}{\input{fig/4/4_1_a_aclms.tikz}}
		\caption{\textit{Learning Curve of the filter, defined as $ 10 \log|e(n)|^2 $}}
		\label{fig:4_1_a_aclms_err}
	\end{subfigure}
	~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
	%(or a blank line to force the subfigure onto a new line)
	\begin{subfigure}[b]{0.49\textwidth}
		\resizebox{\textwidth}{!}{\input{fig/4/4_1_a_aclms_weights.tikz}}
		\caption{\textit{Estimated Weights (both real and imaginary components)}}
		\label{fig:4_1_a_aclms_weights}
	\end{subfigure}
	\caption{\textit{Learning Curve and Estimated Weights from the ACLMS fitler}}
	\label{fig:4_1_a_aclms}
\end{figure}

We can conclude that the CLMS was unable to represent the WLMA properly - the coefficients of $b_2$ which were from the conjugate of $\mathbf{x}$ were nowhere to be seen in the CLMS, but very clear in the ACLMS. The fact that the learning curve of the CLMS showed a large steady state error is evidence that the model was a poor fit for the CLMS.

\subsubsection{Bivariate Wind Data}

We are given wind data for a slow, medium and high wind regime. The data contains wind East-West wind speeds, as well as North-South wind speeds for a given point in time. We represent these in complex form, as $ v[n] = v_{east}[n] + jv_{north}[n] $. Figure \ref{fig:4_1_b_circ} shows the circularity plots for all three regimes of wind. All of the circularity plots appear to be fairly circular. The low wind seems to have a narrower Real set of data than it does in the Imaginary plane. The high wind appears to be shofted off centre, but also has significantly higher amplitude than the low wind (as we would expect!). It appears to have some correlation, observed on the positive real and negative imaginary axes.

\begin{figure}[h]
	\centering 
	\resizebox{\textwidth}{!}{\input{fig/4/4_1_b_circularity.tikz}}
	\caption{\textit{Circularity Plots for Wind Data}}
	\label{fig:4_1_b_circ}
\end{figure}

With the 5000 data points for each wind regime, we can test out filters to predict the next state of the wind. Figure \ref{fig:4_1_b_order} shows the different regimes tested against both the CLMS and ACLMS filters, with varying model orders. As defined in Section \ref{sec:3_3_a}, we use the Mean Squared Prediction Error to find the optimal filter and order. Although there will be some error during setup as the filter coefficients initialise, this is common to all filters and thus will be considered part of the error. The Low Wind demonstrates good performance, with an MSPE of -19dB at a minimum, with it tending to -22dB as the order increases. The ACLMS appears to outperform the CLMS, although only marginally. Medium Wind has a minimum (ie optimum filter length) at 4, using the ACLMS. It obtains an MSPE of around -12.3dB. Interestingly, the CLMS outperforms the ACLMS at values higher than this. The high wind is interesting, as for high model orders the MSPE is very large (hundreds of dB). With an ACLMS model order of 1, an MSPE of -6.25dB is acheived, which represents reasonable performance, especially when considering what happens with a larger order filter!

In conclusion, we are able to model all three systems effectively (all with an MSPE of -6dB or better) in a next value predictor set up, using widely linear filtering. We find that in all three cases the ACLMS does outperform the CLMS, at least for the optimum values (the minimum of these plots) which we are interested in.

\begin{figure}[h]
	\centering
	\setlength\figureheight{0.2\textwidth}
	\setlength\figurewidth{0.2\textwidth}
	\resizebox{\textwidth}{!}{\input{fig/4/4_1_b_orders.tikz}}
	\caption{\textit{Prediction Error Estimates for varying Model Orders}}
	\label{fig:4_1_b_order}
\end{figure}

% \begin{equation}
% S = \frac{1}{N}X' X'^T
% \end{equation}


%  \begin{figure}[h]
%  	\centering 
% 	\resizebox{0.6\textwidth}{!}{\input{q5/q5_cum.tikz}}
%   	\caption{\textit{Cumulative representation of the variance of each eigenvector}}
%   	\label{fig:q5_4}
%  \end{figure}

% \begin{figure}[h]
% 	\centering 
%  	\setlength\figureheight{0.4\textwidth}
% 	\setlength\figurewidth{0.7\textwidth} 
%  	\input{p_1/1.tikz}
%  	\caption{\textit{The four randomly generated subsets}}
%  	\label{fig:q1}
% \end{figure}


%  \begin{figure}[h]
%          \centering
%          \begin{subfigure}[b]{0.45\textwidth}
%             \resizebox{\textwidth}{!}{\input{part_4/q8_num_1.tikz}}
%   			\caption{\textit{1 Tree}}
%          \end{subfigure}
%          ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
%           %(or a blank line to force the subfigure onto a new line)
%          \begin{subfigure}[b]{0.45\textwidth}
%             \resizebox{\textwidth}{!}{\input{part_4/q8_num_3.tikz}}
%   			\caption{\textit{2 Trees}}
%          \end{subfigure}
 		
%          \begin{subfigure}[b]{0.45\textwidth}
%             \resizebox{\textwidth}{!}{\input{part_4/q8_num_5.tikz}}
%   			\caption{\textit{5 Trees}}
%          \end{subfigure}
%          ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
%           %(or a blank line to force the subfigure onto a new line)
%          \begin{subfigure}[b]{0.45\textwidth}
%             \resizebox{\textwidth}{!}{\input{part_4/q8_num_10.tikz}}
%   			\caption{\textit{10 Trees}}
%          \end{subfigure}
         
%          \begin{subfigure}[b]{0.45\textwidth}
%             \resizebox{\textwidth}{!}{\input{part_4/q8_num_20.tikz}}
%   			\caption{\textit{20 Trees}}
%          \end{subfigure}
 		
%  		\label{q9i}
% 		\caption{\textit{Varying the Number of Trees in the Forest}}
%  \end{figure}

\end{document}

